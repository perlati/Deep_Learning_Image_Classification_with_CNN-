![logo_ironhack_blue 7](https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png)

üß† CIFAR-10 Deep Learning: Baseline to Near-SOTA

Framework: TensorFlow/Keras 2.14+
Date: October 2025
Goal: Progressive image classification pipeline from scratch CNN ‚Üí transfer learning ‚Üí optimized ResNet

üìå Project Overview
This project implements three approaches to CIFAR-10 classification, demonstrating the evolution from basic architectures to production-ready models:
 Custom CNN Baseline - Strong from-scratch architecture
 MobileNetV2 Transfer Learning - Efficient fine-tuning approach
 Wide ResNet-28-10 + MixUp - High-accuracy optimized model

üìÅ Repository Structure
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ model.keras (MobileNetV2 Transfer Learning)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ analysis.ipynb

‚öôÔ∏è Setup
bash
# Create environment
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install tensorflow>=2.14 numpy matplotlib scikit-learn

# Verify GPU (optional)
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

**Reproducibility:** All experiments use `seed=42` for NumPy and TensorFlow.

---

## üèóÔ∏è Models & Results

### Model 1: Custom CNN Baseline

**Architecture:**
- 3 convolutional blocks (64‚Üí128‚Üí256 filters)
- Batch normalization + dropout regularization
- Global average pooling + dense head

**Training:**
- SGD + Nesterov momentum (0.9)
- Cosine LR schedule (5-epoch warmup ‚Üí decay over 200 epochs)
- Label smoothing (0.1)
- Data augmentation: random crop, flip

**Performance:**
```
Parameters: ~2M
Test Accuracy: 92.9-93.1%
Training Time: ~2 hours (single GPU)
```

---

### Model 2: MobileNetV2 Transfer Learning

**Approach:**
- **Phase 1** (5 epochs): Freeze backbone, train classification head with Adam
- **Phase 2** (50 epochs): Unfreeze last 50% of layers, fine-tune with SGD + cosine schedule

**Key Details:**
- Input: 32√ó32 ‚Üí resize to 224√ó224
- Preprocessing: `mobilenet_v2.preprocess_input` (expects 0-255 range)
- Early stopping with patience=6

**Performance:**
```
Parameters: ~3.5M (1.8M trainable in phase 2)
Test Accuracy: 93.4-94.5%
Training Time: ~1 hour (single GPU)
```

**Advantages:** Fast training, low overfitting risk, deployment-ready

---

### Model 3: Wide ResNet-28-10 + MixUp

**Architecture:**
- WRN-28-10 with pre-activation residual blocks
- Width multiplier: 10√ó (channels: 160‚Üí320‚Üí640)
- Dropout in residual blocks (0.3)

**Training Recipe:**
- **MixUp:** Beta(0.2, 0.2) for smooth label interpolation
- **Optimizer:** SGD + Nesterov (0.9)
- **LR Schedule:** 5-epoch warmup ‚Üí cosine decay (0.1 ‚Üí 1e-3 over 300 epochs)
- **Regularization:** L2 weight decay (5e-4), data augmentation, early stopping

**Performance:**
```
Parameters: ~36M
Test Accuracy: 96-97% (single pass)
Test Accuracy: 97-98% (with TTA flip)
Training Time: ~6-8 hours (single GPU)

üìä Comparison Summary
Model
Params
Test Acc
Training
Use Case
CNN Baseline
2M
93.1%
2h
Educational, baseline
MobileNetV2
3.5M
94.5%
1h
Production, mobile
WRN-28-10
36M
97%+
8h
Research, benchmarking


üöÄ Running Experiments
bash
# CNN Baseline
python src/train_cnn_baseline.py --epochs 200 --batch 128

# MobileNetV2 Transfer
python src/train_mobilenetv2.py --warmup_epochs 5 --ft_epochs 50

# WRN-28-10 + MixUp
python src/train_wrn28_10.py --epochs 300 --mixup_alpha 0.2
All models save checkpoints to runs/{model_name}/best.keras and log CSVs for analysis.

üî¨ Key Insights
What Works
Transfer learning dramatically reduces training time and overfitting risk
MixUp provides consistent 1-2% accuracy gains with better calibration
Cosine LR + warmup outperforms fixed or step decay schedules
Two-phase fine-tuning (freeze‚Üíunfreeze) prevents catastrophic forgetting
Data Preprocessing Gotchas
MobileNetV2: Keep images in [0,255] before preprocess_input
Custom models: Scale to [0,1] then apply per-channel normalization
Mismatch = 20-30% accuracy drop
Overfitting Prevention
Data augmentation (horizontal flip, crop, rotation)
MixUp or label smoothing
Dropout (0.3-0.5 depending on model size)
Weight decay (L2 regularization)
Early stopping with patience

üìà Performance Ceiling
Why not 100% accuracy?
Label noise: CIFAR-10 contains mislabeled samples
Human performance: ~94-95% agreement
Bayes error: Intrinsic class overlap at 32√ó32 resolution
Realistic ceiling: ~98-99% (current SOTA with Vision Transformers)
Our WRN-28-10 at 97% operates near the practical limit for ResNet-family architectures.

üó∫Ô∏è Future Work

Vision Transformer (ViT) baseline
CutMix and AutoAugment integration
Model calibration analysis (ECE metrics)
ONNX/TFLite export for deployment
Adversarial robustness testing

üìö References

Wide ResNet: Zagoruyko & Komodakis, 2016
MixUp: Zhang et al., 2018
MobileNetV2: Sandler et al., 2018

üìÑ License
MIT License - See LICENSE file for details

üôè Acknowledgments
Models trained on CIFAR-10 dataset (Krizhevsky, 2009). Transfer learning uses ImageNet pre-trained weights from Keras Applications.


